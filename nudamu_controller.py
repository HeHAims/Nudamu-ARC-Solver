import json
import time
import numpy as np
import logging
from typing import Union, Dict, List
from core.pattern_detector import PatternDetector
from core.reasoning_engine import CognitiveReasoner
from data.dataset_loader import DatasetLoader
from utils.validators import Validators

class NudamuController:
    def __init__(self):
        """Initialize controller with enhanced logging and components"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('nudamu.log')
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.detector = PatternDetector()
        self.reasoner = CognitiveReasoner()
        self.logger.info("NudamuController initialized with full fixes")

    def run(self, 
            train_path: str = "data/train.json",
            test_path: str = "data/test.json",
            submission_path: str = "submission.json") -> bool:
        """Main execution method with robust error handling"""
        try:
            self.logger.info(f"Starting processing pipeline")
            
            # Load and validate data
            train_data = self._load_data(train_path, "training")
            test_data = self._load_data(test_path, "test")
            
            if not train_data or not test_data:
                return False

            # Process tasks
            results = self._process_all_tasks(train_data, test_data)
            
            # Save results
            self._save_results(results, submission_path)
            
            return True
        except Exception as e:
            self.logger.error(f"Processing failed: {str(e)}", exc_info=True)
            return False

    def _load_data(self, path: str, data_type: str) -> Union[Dict, None]:
        """Load and validate data with format auto-detection"""
        try:
            raw_data = DatasetLoader.load_json(path)
            if not raw_data:
                self.logger.warning(f"Empty {data_type} dataset at {path}")
                return None
                
            # Convert list format to dictionary if needed
            if isinstance(raw_data, list):
                self.logger.info(f"Converting list format to dictionary for {data_type} data")
                data = {f"task_{i:03d}": [item] for i, item in enumerate(raw_data)}
            else:
                data = raw_data
                
            # Validate all grids
            for task_id, examples in data.items():
                for example in examples:
                    Validators.validate_grid(example["input"])
                    if "output" in example:
                        Validators.validate_grid(example["output"])
            
            self.logger.info(f"Successfully loaded {len(data)} {data_type} tasks")
            return data
            
        except Exception as e:
            self.logger.error(f"Failed to load {data_type} data: {str(e)}")
            return None

    def _process_all_tasks(self, 
                         train_data: Dict, 
                         test_data: Union[Dict, List]) -> Dict:
        """Process all tasks with format handling"""
        submission = []
        benchmark = []
        
        # Normalize test data format
        if isinstance(test_data, list):
            test_data = {f"test_{i:03d}": [{"input": item["input"]}] 
                        for i, item in enumerate(test_data)}
        
        for task_id, test_cases in test_data.items():
            if not test_cases or not test_cases[0].get("input"):
                self.logger.warning(f"Skipping invalid task {task_id}")
                continue
                
            task_result = self._process_task(
                task_id,
                train_data.get(task_id.replace("test_", "task_"), []),
                test_cases[0]["input"]
            )
            
            if task_result:
                submission.append(task_result["submission"])
                benchmark.append(task_result["benchmark"])
        
        return {
            "submission": submission,
            "benchmark": benchmark
        }

    def _process_task(self, 
                     task_id: str, 
                     train_examples: List, 
                     test_input: List) -> Union[Dict, None]:
        """Process a single task with comprehensive scoring"""
        start_time = time.time()
        
        try:
            # Learn patterns from training examples
            learned_patterns = []
            for example in train_examples:
                patterns = self.detector.detect_patterns(
                    np.array(example["input"]),
                    np.array(example["output"])
                )
                if patterns:
                    learned_patterns.append({
                        "patterns": patterns,
                        "confidence": self._calculate_pattern_confidence(patterns)
                    })
            
            # Apply reasoning
            result = self.reasoner.reason_from_learned(
                learned_patterns,
                np.array(test_input)
            )
            
            # Calculate score against training examples
            score = self._calculate_task_score(
                result["output"],
                train_examples,
                [p["type"] for p in learned_patterns[0]["patterns"]] if learned_patterns else []
            )
            
            return {
                "submission": {
                    "task_id": task_id,
                    "output": result["output"].tolist(),
                    "formula": result["formula"],
                    "duration": time.time() - start_time
                },
                "benchmark": {
                    "task_id": task_id,
                    "score": score,
                    "duration": time.time() - start_time,
                    "patterns": [p["type"] for p in learned_patterns[0]["patterns"]] if learned_patterns else []
                }
            }
            
        except Exception as e:
            self.logger.error(f"Task {task_id} failed: {str(e)}")
            return None

    def _calculate_pattern_confidence(self, patterns: List) -> float:
        """Calculate confidence score for a set of patterns"""
        if not patterns:
            return 0.0
            
        # Weight different pattern types
        weights = {
            'complete_transform': 1.3,
            'identity': 1.2,
            'rotation': 1.1,
            'default': 1.0
        }
        
        weighted_sum = sum(
            p.get('confidence', 0) * weights.get(p.get('type'), weights['default'])
            for p in patterns
        )
        
        return min(1.0, weighted_sum / len(patterns))

    def _calculate_task_score(self,
                            actual_output: np.ndarray,
                            train_examples: List,
                            pattern_types: List) -> float:
        """Calculate comprehensive task score"""
        if not train_examples:
            return 0.3  # Default partial credit
            
        best_score = 0.0
        for example in train_examples:
            expected = np.array(example["output"])
            current_score = self._calculate_output_score(actual_output, expected, pattern_types)
            best_score = max(best_score, current_score)
            
        return best_score

    def _calculate_output_score(self,
                              actual: np.ndarray,
                              expected: np.ndarray,
                              pattern_types: List) -> float:
        """Calculate score between actual and expected outputs"""
        # Perfect match
        if np.array_equal(actual, expected):
            return 1.0
            
        # Shape mismatch
        if actual.shape != expected.shape:
            return 0.0
            
        # Base pixel accuracy
        pixel_accuracy = np.sum(actual == expected) / expected.size
        
        # Pattern-based adjustments
        pattern_boost = 0.0
        if 'complete_transform' in pattern_types and np.all(actual == actual[0,0]):
            pattern_boost = 0.3
        elif 'identity' in pattern_types:
            pattern_boost = 0.2
            
        # Structural similarity
        structural_sim = 0.0
        if (actual.max() == expected.max() and 
            actual.min() == expected.min()):
            structural_sim = 0.2
            
        return min(1.0, pixel_accuracy + pattern_boost + structural_sim)

    def _save_results(self, results: Dict, submission_path: str):
        """Save results with validation and backup"""
        try:
            # Save benchmark
            with open("benchmark_results.json", "w") as f:
                json.dump(results["benchmark"], f, indent=2)
                self.logger.info(f"Saved benchmark results for {len(results['benchmark'])} tasks")
            
            # Save submission
            with open(submission_path, "w") as f:
                json.dump(results["submission"], f, indent=2)
                self.logger.info(f"Saved submission to {submission_path}")
                
        except Exception as e:
            self.logger.error(f"Failed to save results: {str(e)}")
            raise

if __name__ == "__main__":
    print("=== Nudamu ARC Solver ===")
    controller = NudamuController()
    
    success = controller.run(
        train_path="data/train.json",
        test_path="data/test.json",
        submission_path="submission.json"
    )
    
    if success:
        print("‚úÖ Processing completed successfully")
        print("üìä Results saved to submission.json and benchmark_results.json")
    else:
        print("‚ùå Processing failed - check nudamu.log for details")